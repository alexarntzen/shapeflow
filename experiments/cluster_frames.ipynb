{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster human walking frames\n",
    "\n",
    "In this experiment we cluster human and running walking human frames into two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from signatureshape.animation import fetch_animations\n",
    "import extratorch as etorch\n",
    "import shapeflow as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reproducible\n",
    "seed = torch.manual_seed(0)\n",
    "\n",
    "# better plotting\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "matplotlib.rcParams.update({\"font.size\": 12})\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and standardize motion capture data. (Same as other notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume all have the same skeleton\n",
    "print(\"Loading mocap data:\")\n",
    "# walk  data\n",
    "walk_subjects = [\"07\", \"08\", \"35\", \"16\"]\n",
    "walk_animations = []\n",
    "walk_desc = []\n",
    "for s in walk_subjects:\n",
    "    for t in fetch_animations(100, subject_file_name=(s + \".asf\")):\n",
    "        if t[2][:4] == \"walk\":\n",
    "            walk_animations.append(t[1])\n",
    "            walk_desc.append(t[2])\n",
    "\n",
    "walk_animations_train_frame = sum(\n",
    "    len(anim.get_frames()) for anim in walk_animations[:18]\n",
    ")\n",
    "\n",
    "# run data\n",
    "run_subjects = [\"09\", \"16\", \"35\"]\n",
    "run_animations = []\n",
    "run_skeletons = []\n",
    "for s in run_subjects:\n",
    "\n",
    "    for t in fetch_animations(100, subject_file_name=(s + \".asf\")):\n",
    "        if t[2][:3] == \"run\":\n",
    "            run_skeletons.append(t[0])\n",
    "            run_animations.append(t[1])\n",
    "\n",
    "print(\"Convert to array:\")\n",
    "walk_angle_array = sf.utils.animation_to_eulers(\n",
    "    walk_animations,\n",
    "    reduce_shape=True,\n",
    "    remove_root=True,\n",
    "    deg2rad=True,\n",
    "    skeleton=run_skeletons[0],\n",
    "    max_frame_count=240,\n",
    ")\n",
    "run_angle_array = sf.utils.animation_to_eulers(\n",
    "    run_animations,\n",
    "    reduce_shape=True,\n",
    "    remove_root=True,\n",
    "    deg2rad=True,\n",
    "    skeleton=run_skeletons[0],\n",
    "    max_frame_count=240,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_angle_tensor_ = torch.tensor(walk_angle_array, dtype=torch.float32)\n",
    "run_angle_tensor_ = torch.tensor(run_angle_array, dtype=torch.float32)\n",
    "wr_angle_tensor_ = torch.cat((walk_angle_tensor_, run_angle_tensor_))\n",
    "\n",
    "# standardize\n",
    "std, mean = torch.std_mean(wr_angle_tensor_, dim=0)\n",
    "wr_angle_tensor_norm = (wr_angle_tensor_ - mean) / std\n",
    "run_angle_tensor_norm = (run_angle_tensor_ - mean) / std\n",
    "walk_angle_tensor_norm = (walk_angle_tensor_ - mean) / std\n",
    "\n",
    "nonzero = torch.argwhere(\n",
    "    torch.sum(torch.abs(torch.diff(wr_angle_tensor_, dim=0)), dim=0) > 0.0\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose subsets of joint angles ( integers $i$, $0\\leq i \\leq 44$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = nonzero[[11, 23, 30]]\n",
    "\n",
    "walk_angle_tensor = walk_angle_tensor_norm[:, chosen]\n",
    "run_angle_tensor = run_angle_tensor_norm[:, chosen]\n",
    "wr_angle_tensor = wr_angle_tensor_norm[:, chosen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = run_skeletons[0]\n",
    "bonelist = []\n",
    "for b in s.bones.items():\n",
    "    for dof in b[1].dof:\n",
    "        bonelist.append(b[0] + \" \" + dof)\n",
    "\n",
    "print(\"Chosen angles:\")\n",
    "[bonelist[i] for i in chosen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make priors\n",
    "Since (pretend) we do not know the class of each observation the estimated prior probability is (0.5 + $\\epsilon$,0.5 - $\\epsilon$) for all observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select number of supervised points\n",
    "supervised_interval = 0\n",
    "\n",
    "run_len = run_angle_tensor.shape[0]\n",
    "walk_len = walk_angle_tensor.shape[0]\n",
    "prior_run = torch.cat((torch.zeros(walk_len), torch.ones(run_len)))\n",
    "\n",
    "# q run, walk\n",
    "q = torch.stack((prior_run, abs(prior_run - 1)), dim=1)\n",
    "\n",
    "priors = q.clone().detach()\n",
    "eps = torch.rand(len(priors)) * 0.1\n",
    "priors[:, 1] = 0.5 + eps\n",
    "priors[:, 0] = 0.5 - eps\n",
    "\n",
    "if supervised_interval > 0:\n",
    "    priors[::supervised_interval] = q[::supervised_interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put data into datasets for training\n",
    "data_walk = torch.utils.data.TensorDataset(walk_angle_tensor)\n",
    "data_run = torch.utils.data.TensorDataset(run_angle_tensor)\n",
    "data = torch.utils.data.TensorDataset(wr_angle_tensor, priors.clone().detach(), priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering experiments:\n",
    "\n",
    "First, set up model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "DIR = \"../figures/cluster_frames/\"\n",
    "SET_NAME = \"cont_euler_3\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "########\n",
    "\n",
    "event_shape = data[0][0].shape\n",
    "base_dist = dist.Independent(\n",
    "    dist.Normal(loc=torch.zeros(event_shape), scale=torch.ones(event_shape)), 1\n",
    ")\n",
    "\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": sf.nf.get_flow,\n",
    "    \"get_transform\": sf.transforms.NDETransform,\n",
    "    \"base_dist\": base_dist,\n",
    "    \"get_net\": etorch.models.FFNN,\n",
    "    \"activation\": \"tanh\",\n",
    "    \"inverse_model\": True,\n",
    "    \"num_flows\": 2,\n",
    "    \"sensitivity\": \"adjoint\",\n",
    "    \"neurons\": [16],\n",
    "    \"n_hidden_layers\": [4],\n",
    "}\n",
    "\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [5000],\n",
    "    \"compute_loss\": [sf.nf.get_monte_carlo_conditional_dkl_loss()],\n",
    "    \"verbose\": True,\n",
    "    \"optimizer\": [\"ADAM\"],\n",
    "    \"num_epochs\": [50],\n",
    "    \"learning_rate\": [0.01],\n",
    "    \"lr_scheduler\": [lr_scheduler],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual training with model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_iter = etorch.create_subdictionary_iterator(MODEL_PARAMS, product=True)\n",
    "training_params_iter = etorch.create_subdictionary_iterator(\n",
    "    TRAINING_PARAMS, product=True\n",
    ")\n",
    "\n",
    "cv_results = etorch.k_fold_cv_grid(\n",
    "    model_params=model_params_iter,\n",
    "    fit=etorch.fit_module,\n",
    "    training_params=training_params_iter,\n",
    "    data=data,\n",
    "    verbose=True,\n",
    "    copy_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etorch.plotting.plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    ")\n",
    "models = cv_results[\"models\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose joint angels to show in plot\n",
    "i, j = 0, 1\n",
    "\n",
    "# plot two dimensions\n",
    "sample0 = models[0].sample([1000]).detach()\n",
    "sample1 = models[1].sample([1000]).detach()\n",
    "df0 = pd.DataFrame({\"x\": sample0[:, j], \"y\": sample0[:, i], \"Flow\": [0] * len(sample0)})\n",
    "df1 = pd.DataFrame({\"x\": sample1[:, j], \"y\": sample1[:, i], \"Flow\": [1] * len(sample1)})\n",
    "df = pd.concat((df0, df1), axis=0)\n",
    "df.index = range(len(df))\n",
    "sns.displot(df, x=\"x\", y=\"y\", hue=\"Flow\", kind=\"kde\")\n",
    "run_point = data_run[:][0]\n",
    "walk_points = data_walk[:][0]\n",
    "plt.scatter(walk_points[:, j], walk_points[:, i], color=\"green\", label=\"Walk samples\")\n",
    "plt.scatter(\n",
    "    run_point[:, j], run_point[:, i], marker=\"x\", color=\"grey\", label=\"Run samples\"\n",
    ")\n",
    "plt.xlabel(bonelist[chosen[j]])\n",
    "plt.ylabel(bonelist[chosen[i]])\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(PATH_FIGURES + \"/axis_distribution.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the final clustering into two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data = data_walk[:][0]\n",
    "print(\"Walk data:\")\n",
    "print(\n",
    "    \"Class 1:\",\n",
    "    torch.sum(models[0].log_prob(motion_data) < models[1].log_prob(motion_data)).item(),\n",
    "    \"Class 2:\",\n",
    "    torch.sum(models[0].log_prob(motion_data) > models[1].log_prob(motion_data)).item(),\n",
    ")\n",
    "\n",
    "motion_data = data_run[:][0]\n",
    "print(\"Run data:\")\n",
    "print(\n",
    "    \"Class 1 :\",\n",
    "    torch.sum(models[0].log_prob(motion_data) < models[1].log_prob(motion_data)).item(),\n",
    "    \"Class 2:\",\n",
    "    torch.sum(models[0].log_prob(motion_data) > models[1].log_prob(motion_data)).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable selection\n",
    "\n",
    "Here we test which dimension that clusters the best. That is, with the lowest conditional entropy.\n",
    "\n",
    "To do this we run a model for each dimension (joint angle) and compare the conditional entropy.\n",
    "\n",
    "First we choose the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"../figures/cluster_frames\"\n",
    "SET_NAME = \"cont_euler_4\"\n",
    "PATH_FIGURES_TRIAL = os.path.join(DIR, \"trial\", SET_NAME)\n",
    "\n",
    "event_shape = (1,)\n",
    "base_dist = dist.Independent(\n",
    "    dist.Normal(loc=torch.zeros(event_shape), scale=torch.ones(event_shape)), 1\n",
    ")\n",
    "\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, mode=\"min\", factor=0.5, patience=5, verbose=False\n",
    ")\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": sf.nf.get_flow,\n",
    "    \"get_transform\": sf.transforms.NDETransform,\n",
    "    \"base_dist\": base_dist,\n",
    "    \"get_net\": etorch.models.FFNN,\n",
    "    \"activation\": \"tanh\",\n",
    "    \"inverse_model\": True,\n",
    "    \"num_flows\": 2,\n",
    "    \"sensitivity\": \"adjoint\",\n",
    "    \"neurons\": [16],\n",
    "    \"n_hidden_layers\": [4],\n",
    "}\n",
    "\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [5000],\n",
    "    \"compute_loss\": [sf.nf.get_monte_carlo_conditional_dkl_loss()],\n",
    "    \"optimizer\": [\"ADAM\"],\n",
    "    \"num_epochs\": [100],\n",
    "    \"learning_rate\": [0.01],\n",
    "    \"compute_log\": sf.nf.get_cluster_log,\n",
    "    \"lr_scheduler\": [lr_scheduler],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for each bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_j = []\n",
    "# remove and False to start\n",
    "do_variable_selection = False\n",
    "if do_variable_selection:\n",
    "    for joint in tqdm(nonzero):\n",
    "        # create iterators\n",
    "        model_params_iter = etorch.create_subdictionary_iterator(\n",
    "            MODEL_PARAMS, product=True\n",
    "        )\n",
    "        training_params_iter = etorch.create_subdictionary_iterator(\n",
    "            TRAINING_PARAMS, product=True\n",
    "        )\n",
    "        data = torch.utils.data.TensorDataset(\n",
    "            wr_angle_tensor_norm[::10, joint : joint + 1],\n",
    "            priors[::10].clone().detach(),\n",
    "            priors[::10],\n",
    "        )\n",
    "        cv_results_j = etorch.k_fold_cv_grid(\n",
    "            model_params=model_params_iter,\n",
    "            fit=etorch.fit_module,\n",
    "            training_params=training_params_iter,\n",
    "            data=data,\n",
    "            verbose=True,\n",
    "            copy_data=True,\n",
    "            trials=5,\n",
    "        )\n",
    "        etorch.plotting.plot_result(\n",
    "            path_figures=PATH_FIGURES_TRIAL + f\"_{joint}\",\n",
    "            **cv_results_j,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make table (data frame) for variable selection\n",
    "dfs = []\n",
    "for i in nonzero:\n",
    "    for j in range(5):\n",
    "        row = pd.read_csv(PATH_FIGURES_TRIAL + f\"_{i}/history_plot_t{j}_c0_f0.csv\")[\n",
    "            99:100\n",
    "        ]\n",
    "        row[\"trial\"] = j\n",
    "        row[\"joint\"] = i.item()\n",
    "        dfs.append(row)\n",
    "loss_df = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the conditional entropy for each joint angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    data=loss_df,\n",
    "    kind=\"box\",\n",
    "    aspect=5,\n",
    "    x=\"joint\",\n",
    "    y=\"Conditional entropy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the training loss for each . This is the part of KL divergence that we try to minimize, but it is not conditional entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    data=loss_df,\n",
    "    kind=\"box\",\n",
    "    aspect=5,\n",
    "    x=\"joint\",\n",
    "    y=\"Training loss\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
