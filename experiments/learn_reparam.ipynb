{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import flowtorch as ft\n",
    "\n",
    "import nflows\n",
    "import normflow as nf\n",
    "\n",
    "from signatureshape.animation import fetch_animation_id_set, fetch_animations\n",
    "from signatureshape.animation.src.mayavi_animate import mayavi_animate\n",
    "\n",
    "from deepthermal.validation import (\n",
    "    create_subdictionary_iterator,\n",
    "    k_fold_cv_grid,\n",
    "    add_dictionary_iterators,\n",
    ")\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN\n",
    "from deepthermal.plotting import plot_result\n",
    "import experiments.curves as c1\n",
    "\n",
    "import shapeflow as sf\n",
    "\n",
    "# make reproducible\n",
    "seed = torch.manual_seed(0)\n",
    "\n",
    "# better formats\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data as so3\n",
    "# we assume all have the same skeleton\n",
    "\n",
    "N = 32\n",
    "\n",
    "times = torch.linspace(0, 1, N)\n",
    "\n",
    "reparam_list = []\n",
    "\n",
    "for i in tqdm(range(5000)):\n",
    "    c2_data_reparam = sf.reparam.reparam_curve(\n",
    "        curve=c1.c_1, times=times, max_step=N // 2\n",
    "    )\n",
    "    reparam_list.append(c2_data_reparam)\n",
    "\n",
    "c2_reparams = torch.as_tensor(np.stack(reparam_list))\n",
    "c2_reparams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(c2_reparams[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.utils.data.TensorDataset(c2_reparams[::2].float())\n",
    "data_val = torch.utils.data.TensorDataset(c2_reparams[1::2].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ft.bijectors.Autoregressive(\n",
    "    ft.parameters.DenseAutoregressive(hidden_dims=(N, N))\n",
    ")\n",
    "b = test(shape=(2))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "DIR = \"../figures/c1_c2_repara/\"\n",
    "SET_NAME = \"walk_residual\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "FOLDS = 5\n",
    "#######\n",
    "\n",
    "event_shape = data[0][0].shape\n",
    "base_dist = dist.Independent(\n",
    "    dist.Normal(loc=torch.zeros(event_shape), scale=torch.ones(event_shape)), 1\n",
    ")\n",
    "\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, mode=\"min\", factor=0.5, patience=50, verbose=True\n",
    ")\n",
    "\n",
    "# def get_flow(ji)\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": [ft.distributions.Flow],\n",
    "    \"bijector\": [\n",
    "        ft.bijectors.AffineAutoregressive(\n",
    "            ft.parameters.DenseAutoregressive(hidden_dims=(N, N))\n",
    "        )\n",
    "    ],\n",
    "    # \"input_dim\" : [event_shape[0]],\n",
    "    # \"inverse_model\": [False],\n",
    "    # \"compose\": [True],\n",
    "}\n",
    "num_layers = 2\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"base_dist\": [base_dist],\n",
    "    # \"hidden_dims\": [(N,N)],\n",
    "    # \"hidden_layers\": [[2] * num_layers],\n",
    "    # \"n_exact_terms\": [[4] * num_layers],\n",
    "    # \"n_samples\": [[10] * num_layers],\n",
    "}\n",
    "\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [1000],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [sf.monte_carlo_dkl_loss],\n",
    "    \"post_batch\": [sf.get_post_step_lipchitz(5)],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"verbose_interval\": [20],\n",
    "    \"optimizer\": [\"ADAM\"],\n",
    "    \"num_epochs\": [300],\n",
    "    \"learning_rate\": [0.1],\n",
    "    \"lr_scheduler\": [lr_scheduler],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter_1)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(\n",
    "    TRAINING_PARAMS_EXPERIMENT, product=False\n",
    ")\n",
    "exp_training_params_iter = add_dictionary_iterators(\n",
    "    training_exp_iter, training_params_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    val_data=data_val,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=1,\n",
    "    partial=True,\n",
    "    shuffle_folds=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that the wrapper works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    # plot_function=plot_model_1d,\n",
    "    # function_kwargs=plot_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = cv_results[\"models\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(flow, \"Flow_frames\" + SET_NAME + \"2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func = lambda t: np.cos(t * 2 * np.pi)\n",
    "for i in tqdm(range(5000)):\n",
    "    c2_data_reparam = sf.reparam.reparam_curve(\n",
    "        curve_data=test_func, times=times, max_step=N // 2\n",
    "    )\n",
    "    reparam_list.append(c2_data_reparam)\n",
    "\n",
    "c2_reparams_2 = torch.as_tensor(np.stack(reparam_list)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = base_dist.sample([100])\n",
    "print(\"Log vals:\")\n",
    "\n",
    "print(\"Noise :\", flow.log_prob(noise[0:1]).mean().item())\n",
    "print(\"Cos dat :\", flow.log_prob(c2_reparams_2[0:5000]).mean().item())\n",
    "print(\"Train data:\", flow.log_prob(data[:][0]).mean().item())\n",
    "print(\"Train data:\", flow.log_prob(data_val[:][0]).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[115][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_two motions\n",
    "# x_first_frame = torch.tensor(np.deg2rad(walk_animations[0].to_numpy_array())[3:].T).float()[0:1]\n",
    "# x_first_frame = torch.tensor(np.deg2rad(walk_animations[0].to_numpy_array())[3:].T).float()[0:1]\n",
    "i, j = 0, 1200\n",
    "x_first_frame = data[i : i + 1][0]\n",
    "x_second_frame = data[j : j + 1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate\n",
    "z_first_walk = flow.bijector.inverse(x_first_frame)\n",
    "z_second_walk = flow.bijector.inverse(x_second_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = torch.cat((torch.linspace(1, 0, 120), torch.linspace(0, 1, 120)))\n",
    "x_interpolated = torch.cat(\n",
    "    [flow.bijector.forward(z_first_walk * w + z_second_walk * (1 - w)) for w in w_list]\n",
    ")\n",
    "x_interpolated_test = torch.cat(\n",
    "    [x_first_frame * w + x_second_frame * (1 - w) for w in w_list]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skel = copy.deepcopy(run_skeletons[0])\n",
    "\n",
    "anim_test = copy.deepcopy(walk_animations[0])\n",
    "anim_first = copy.deepcopy(walk_animations[0])\n",
    "anim_second = copy.deepcopy(walk_animations[0])\n",
    "anim_first.from_numpy_array(sf.utils.data_to_motion_array(x_first_frame))\n",
    "anim_second.from_numpy_array(sf.utils.data_to_motion_array(x_second_frame))\n",
    "anim_test.from_numpy_array(sf.utils.data_to_motion_array(x_interpolated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = mayavi_animate(\n",
    "    skel,\n",
    "    anim_first,\n",
    "    offset=[0, 0, 0],\n",
    "    continuous=False,\n",
    "    fixed_cam=False,\n",
    "    frame_limit=-1,\n",
    "    save_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = mayavi_animate(\n",
    "    skel,\n",
    "    anim_second,\n",
    "    offset=[0, 0, 0],\n",
    "    continuous=False,\n",
    "    fixed_cam=False,\n",
    "    frame_limit=-1,\n",
    "    save_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = mayavi_animate(\n",
    "    skel,\n",
    "    anim_test,\n",
    "    offset=[0, 0, 0],\n",
    "    continuous=False,\n",
    "    fixed_cam=False,\n",
    "    frame_limit=-1,\n",
    "    save_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
