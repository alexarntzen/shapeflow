{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from flowtorch.distributions import Flow\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import matplotlib\n",
    "import shapeflow as sf\n",
    "import sklearn.datasets as datasets\n",
    "import seaborn as sns\n",
    "import extratorch as etorch\n",
    "from signatureshape.animation.src.mayavi_animate import mayavi_animate\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 12})\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# make reproducible\n",
    "seed = torch.manual_seed(0)\n",
    "\n",
    "# better formats\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datasets.make_moons(8 * 1024, noise=0.05)\n",
    "\n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "q = np.stack((y, np.abs(y - 1)), axis=-1)\n",
    "\n",
    "# standardize\n",
    "x_tensor = torch.as_tensor((x - mean) / std, dtype=torch.float32)\n",
    "\n",
    "plt.scatter(\n",
    "    x_tensor[:, 0],\n",
    "    x_tensor[:, 1],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data\n",
    "data = torch.utils.data.TensorDataset(\n",
    "    x_tensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "DIR = \"../figures/interpolate_moons/\"\n",
    "SET_NAME = \"cnf_2\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "event_shape = data[0][0].shape\n",
    "base_dist = dist.MultivariateNormal(\n",
    "    torch.zeros(event_shape[0]), torch.eye(event_shape[0])\n",
    ")\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "stack = 4\n",
    "flows = sf.nf.get_flow(\n",
    "    base_dist=base_dist,\n",
    "    inverse_model=True,\n",
    "    compose=True,\n",
    "    get_transform=sf.transforms.NDETransform,\n",
    "    get_net=[etorch.FFNN] * stack,\n",
    "    activation=[\"tanh\"] * stack,\n",
    "    n_hidden_layers=[3] * stack,\n",
    "    neurons=[8] * stack,\n",
    "    trace_estimator=[\"autograd_trace\"] * stack,\n",
    ")\n",
    "# flows = sf.nf.get_flow(\n",
    "#     base_dist=base_dist,\n",
    "#     inverse_model=True,\n",
    "#     compose=False,\n",
    "#     get_transform=sf.transforms.NDETransform,\n",
    "#     get_net=etorch.FFNN,\n",
    "#     activation=\"tanh\",\n",
    "#     n_hidden_layers=3,\n",
    "#     neurons=16,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = etorch.fit_module(\n",
    "    model=flows,\n",
    "    batch_size=256,\n",
    "    compute_loss=sf.nf.monte_carlo_dkl_loss,\n",
    "    optimizer=\"ADAM\",\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.01,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    data=data,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, hist = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that the wrapper workps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = base_dist.sample([100])\n",
    "print(\"Log vals=\")\n",
    "\n",
    "print(\"Noise :\", model.log_prob(noise[0:1]).mean().item())\n",
    "print(\"Trian data:\", model.log_prob(data[:][0]).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = (torch.tensor([[2.0, 0.25]]) - mean) / std\n",
    "p2 = (torch.tensor([[0.0, 0.25]]) - mean) / std\n",
    "\n",
    "z1 = model.rnormalize(p1)\n",
    "z2 = model.rnormalize(p2)\n",
    "\n",
    "line = torch.unsqueeze(torch.linspace(0, 1, 200), 1)\n",
    "interp_line_z = z1 * line + z2 * (1 - line)\n",
    "interp_line_x = model.bijector.forward(interp_line_z).detach()\n",
    "interp_line_x_naive = p1 * line + p2 * (1 - line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "t_points = model.sample([10000]).detach().numpy()\n",
    "ax.scatter(\n",
    "    t_points[:, 0],\n",
    "    t_points[:, 1],\n",
    "    marker=\".\",\n",
    "    alpha=0.3,\n",
    "    color=\"grey\",\n",
    "    label=\"Generated samples\",\n",
    ")\n",
    "ax.plot(\n",
    "    interp_line_x[:, 0],\n",
    "    interp_line_x[:, 1],\n",
    "    \"o\",\n",
    "    ls=\"-\",\n",
    "    markevery=10,\n",
    "    label=\"Latent space interp.\",\n",
    "    lw=2,\n",
    ")\n",
    "ax.plot(\n",
    "    interp_line_x_naive[:, 0],\n",
    "    interp_line_x_naive[:, 1],\n",
    "    \"-.\",\n",
    "    lw=2,\n",
    "    label=\"Feature space interp.\",\n",
    ")\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "ax.axis(\"off\")\n",
    "ax.legend()\n",
    "fig.savefig(\n",
    "    os.path.join(\n",
    "        PATH_FIGURES,\n",
    "        \"interpolation_path.pdf\",\n",
    "    ),\n",
    "    bbox_inches=\"tight\",\n",
    "    pad_inches=0,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_line_z_naive = model.rnormalize(interp_line_x_naive).detach()\n",
    "t = np.linspace(0, 1, 200)\n",
    "plt.plot(t, model.log_prob(interp_line_x).detach(), \"-\", label=\"Latent space interp.\")\n",
    "plt.plot(\n",
    "    t, model.log_prob(interp_line_x_naive).detach(), \"-.\", label=\"Feature space interp.\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$\\log p_{T(Z)}$\")\n",
    "plt.savefig(\n",
    "    os.path.join(\n",
    "        PATH_FIGURES,\n",
    "        \"interpolation_log_prob.pdf\",\n",
    "    ),\n",
    "    bbox_inches=\"tight\",\n",
    "    pad_inches=0,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
