{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "from signatureshape.animation import fetch_animation_id_set, fetch_animations\n",
    "from signatureshape.animation.src.mayavi_animate import mayavi_animate\n",
    "\n",
    "import extratorch as etorch\n",
    "import shapeflow as sf\n",
    "\n",
    "# make reproducible\n",
    "seed = torch.manual_seed(0)\n",
    "\n",
    "# better formats\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data as eulers\n",
    "# we assume all have the same skeleton\n",
    "print(\"Loading mocap data:\")\n",
    "# walk  data\n",
    "walk_subjects = [\"07\", \"08\", \"35\", \"16\"]\n",
    "walk_animations = []\n",
    "for s in walk_subjects:\n",
    "    for t in fetch_animations(100, subject_file_name=(s + \".asf\")):\n",
    "        if t[2][:4] == \"walk\":\n",
    "            walk_animations.append(t[1])\n",
    "\n",
    "# run data\n",
    "run_subjects = [\"09\", \"16\", \"35\"]\n",
    "run_animations = []\n",
    "run_skeletons = []\n",
    "for s in run_subjects:\n",
    "    for t in fetch_animations(100, subject_file_name=(s + \".asf\")):\n",
    "        if t[2][:3] == \"run\":\n",
    "            run_skeletons.append(t[0])\n",
    "            run_animations.append(t[1])\n",
    "\n",
    "print(\"Convert to array:\")\n",
    "walk_angle_array = sf.utils.animation_to_eulers(\n",
    "    walk_animations,\n",
    "    reduce_shape=False,\n",
    "    remove_root=True,\n",
    "    deg2rad=True,\n",
    "    max_frame_count=240,\n",
    ")\n",
    "run_angle_array = sf.utils.animation_to_eulers(\n",
    "    run_animations,\n",
    "    reduce_shape=False,\n",
    "    remove_root=True,\n",
    "    deg2rad=True,\n",
    "    max_frame_count=240,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skel2, anim, desc = fetch_animations(1, subject_file_name=run_subjects[0]+\".asf\")[0]\n",
    "# for bone_name, bone_obj in skel2.bones.items():\n",
    "#     pass\n",
    "#     print(bone_name, \":\")\n",
    "#     print([dof for dof in bone_obj.dof], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors on the form\n",
    "# (motion, time, joint)\n",
    "walk_angle_tensor = torch.tensor(walk_angle_array, dtype=torch.float32)\n",
    "run_angle_tensor = torch.tensor(run_angle_array, dtype=torch.float32)\n",
    "\n",
    "pre_shape_walk = walk_angle_tensor.shape\n",
    "pre_shape_run = run_angle_tensor.shape\n",
    "num_frames = min(pre_shape_walk[1], pre_shape_run[1])\n",
    "\n",
    "nonzero = torch.argwhere(\n",
    "    torch.sum(torch.abs(torch.diff(walk_angle_tensor, dim=1)), dim=[0, 1]) > 0.0\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert best shapes here\n",
    "skip_frames = 12\n",
    "choosen = nonzero[[24, 26, 33, 34]]\n",
    "\n",
    "# cut and reduce frames\n",
    "walk_angles = walk_angle_tensor[\n",
    "    :, :num_frames:skip_frames, choosen\n",
    "]  # .reshape(post_shape_walk)\n",
    "run_angles = run_angle_tensor[\n",
    "    :, :num_frames:skip_frames, choosen\n",
    "]  # .reshape(post_shape_run\n",
    "wr_angles = torch.cat((walk_angles, run_angles))\n",
    "\n",
    "animation_shape = wr_angles.shape[-2:]\n",
    "animation_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std, mean = torch.std_mean(wr_angles, dim=0)\n",
    "wr_angles_norm = (wr_angles - mean) / std\n",
    "run_angles_norm = (run_angles - mean) / std\n",
    "walk_angles_norm = (walk_angles - mean) / std\n",
    "std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape\n",
    "flatten = True\n",
    "add_channel = False\n",
    "make_frames = False\n",
    "orig_shape_walk = walk_angles.shape\n",
    "orig_shape_run = run_angles.shape\n",
    "orig_shape_wr = wr_angles.shape\n",
    "\n",
    "if add_channel:\n",
    "    walk_angles_nr = torch.unsqueeze(walk_angles_norm, 1)\n",
    "    run_angles_nr = torch.unsqueeze(walk_angles_norm, 1)\n",
    "    wr_angles_nr = torch.unsqueeze(wr_angles_norm, 1)\n",
    "elif flatten:\n",
    "    walk_angles_nr = walk_angles_norm.reshape(\n",
    "        orig_shape_walk[0], orig_shape_walk[1] * orig_shape_walk[2]\n",
    "    )\n",
    "    run_angles_nr = run_angles_norm.reshape(\n",
    "        orig_shape_run[0], orig_shape_run[1] * orig_shape_run[2]\n",
    "    )\n",
    "    wr_angles_nr = wr_angles_norm.reshape(\n",
    "        orig_shape_wr[0], orig_shape_wr[1] * orig_shape_wr[2]\n",
    "    )\n",
    "elif make_frames:\n",
    "    walk_angles_nr = walk_angles_norm.reshape(\n",
    "        orig_shape_walk[0] * orig_shape_walk[1], orig_shape_walk[2]\n",
    "    )\n",
    "    run_angles_nr = run_angles_norm.reshape(\n",
    "        orig_shape_run[0] * orig_shape_run[1], orig_shape_run[2]\n",
    "    )\n",
    "    wr_angles_nr = wr_angles_norm.reshape(\n",
    "        orig_shape_wr[0] * orig_shape_wr[1], orig_shape_wr[2]\n",
    "    )\n",
    "wr_angles_nr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_priors = torch.cat(\n",
    "    (torch.ones(len(walk_angles_nr)), torch.zeros(len(run_angles_nr)))\n",
    ")\n",
    "run_priors = abs(walk_priors - 1)\n",
    "q = torch.stack((walk_priors, run_priors), dim=1)\n",
    "\n",
    "priors = np.zeros_like(q)\n",
    "eps = np.random.rand(len(priors)) * 0.1\n",
    "\n",
    "#  priors with equal probabiliy\n",
    "priors[:] = 0.5\n",
    "priors[:, 1] += eps\n",
    "priors[:, 0] -= eps\n",
    "walk_priors.shape, priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk_angles_rev  = torch.swapaxes(walk_angles, 2, 1)\n",
    "data = torch.utils.data.TensorDataset(wr_angles_nr, priors, priors)\n",
    "data_walk = torch.utils.data.TensorDataset(walk_angles_nr)\n",
    "data_run = torch.utils.data.TensorDataset(run_angles_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "DIR = \"../figures/cluster_shape/\"\n",
    "SET_NAME = \"dim_selection_2\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "event_shape = data[0][0].shape\n",
    "base_dist = dist.Independent(\n",
    "    dist.Normal(loc=torch.zeros(event_shape), scale=torch.ones(event_shape)), 1\n",
    ")\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, mode=\"min\", factor=0.5, patience=50, verbose=True\n",
    ")\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": sf.nf.get_flow,\n",
    "    \"get_transform\": sf.transforms.NDETransform,\n",
    "    \"base_dist\": base_dist,\n",
    "    \"get_net\": sf.models.CNN2D,\n",
    "    \"activation\": \"tanh\",\n",
    "    \"inverse_model\": True,\n",
    "    \"num_flows\": 2,\n",
    "    \"sensitivity\": \"autograd\"\n",
    "    # \"trace_estimator\" : \"hutch_trace\"\n",
    "}\n",
    "EXTRA_M_PARAMS = {\n",
    "    \"kernel_size\": (3, animation_shape[-1] - 1),\n",
    "    \"internal_shape\": animation_shape,\n",
    "    \"n_hidden_layers\": [4],\n",
    "}\n",
    "\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [50],\n",
    "    \"compute_loss\": [sf.nf.get_monte_carlo_elbo_loss(epsilon=1)],\n",
    "    \"verbose\": True,\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "EXTRA_T_PARAMS = {\n",
    "    \"optimizer\": [\"ADAM\"],\n",
    "    \"num_epochs\": [100],\n",
    "    \"learning_rate\": [0.01],\n",
    "    \"lr_scheduler\": [lr_scheduler],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterators\n",
    "m_temp_1 = etorch.create_subdictionary_iterator(MODEL_PARAMS, product=True)\n",
    "m_temp_2 = etorch.create_subdictionary_iterator(EXTRA_M_PARAMS, product=False)\n",
    "model_params_iter = etorch.add_dictionary_iterators(m_temp_1, m_temp_2, product=True)\n",
    "\n",
    "t_temp_1 = etorch.create_subdictionary_iterator(TRAINING_PARAMS, product=True)\n",
    "t_temp_2 = etorch.create_subdictionary_iterator(EXTRA_T_PARAMS, product=False)\n",
    "training_params_iter = etorch.add_dictionary_iterators(t_temp_1, t_temp_2, product=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = etorch.k_fold_cv_grid(\n",
    "    fit=etorch.fit_module,\n",
    "    model_params=model_params_iter,\n",
    "    training_params=training_params_iter,\n",
    "    data=data,\n",
    "    verbose=True,\n",
    "    partial=True,\n",
    "    shuffle_folds=False,\n",
    "    copy_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etorch.plotting.plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    ")\n",
    "models = cv_results[\"models\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data = data_walk[:][0]\n",
    "print(\"Walk data:\")\n",
    "print(\n",
    "    \"Class 1:\",\n",
    "    torch.sum(models[0].log_prob(motion_data) < models[1].log_prob(motion_data)).item(),\n",
    "    \"Class 2:\",\n",
    "    torch.sum(models[0].log_prob(motion_data) > models[1].log_prob(motion_data)).item(),\n",
    ")\n",
    "\n",
    "motion_data = data_run[:][0]\n",
    "print(\"Run data:\")\n",
    "print(\n",
    "    \"Class 1 :\",\n",
    "    torch.sum(models[0].log_prob(motion_data) < models[1].log_prob(motion_data)).item(),\n",
    "    \"Class 2:\",\n",
    "    torch.sum(models[0].log_prob(motion_data) > models[1].log_prob(motion_data)).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_cond_c = torch.zeros(len(data), len(models))\n",
    "with torch.no_grad():\n",
    "    for c, model in enumerate(models):\n",
    "        log_p_cond_c[:, c] = model.log_prob(data[:][0])\n",
    "\n",
    "print(\"q log p: \", (q * log_p_cond_c).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
